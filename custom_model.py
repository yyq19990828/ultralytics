import re
from turtle import back
from ultralytics.nn.tasks import BaseModel, DetectionModel, yaml_model_load, parse_model
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import contextlib
from copy import deepcopy
from pathlib import Path

import torch
import torch.nn as nn

from ultralytics.nn.modules import (
    AIFI,
    C1,
    C2,
    C3,
    C3TR,
    ELAN1,
    OBB,
    PSA,
    SPP,
    SPPELAN,
    SPPF,
    AConv,
    ADown,
    Bottleneck,
    BottleneckCSP,
    C2f,
    C2fAttn,
    C2fCIB,
    C3Ghost,
    C3x,
    CBFuse,
    CBLinear,
    Classify,
    Concat,
    Conv,
    Conv2,
    ConvTranspose,
    Detect,
    DWConv,
    DWConvTranspose2d,
    Focus,
    GhostBottleneck,
    GhostConv,
    HGBlock,
    HGStem,
    ImagePoolingAttn,
    Pose,
    RepC3,
    RepConv,
    RepNCSPELAN4,
    RepVGGDW,
    ResNetLayer,
    RTDETRDecoder,
    SCDown,
    Segment,
    WorldDetect,
    v10Detect,
)
from ultralytics.utils import DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, colorstr, emojis, yaml_load
from ultralytics.utils.checks import check_requirements, check_suffix, check_yaml
from ultralytics.utils.loss import (
    E2EDetectLoss,
    v8ClassificationLoss,
    v8DetectionLoss,
    v8OBBLoss,
    v8PoseLoss,
    v8SegmentationLoss,
)
from ultralytics.utils.ops import make_divisible
from ultralytics.utils.plotting import feature_visualization
from ultralytics.utils.torch_utils import (
    fuse_conv_and_bn,
    fuse_deconv_and_bn,
    initialize_weights,
    intersect_dicts,
    model_info,
    scale_img,
    time_sync,
)

try:
    import thop
except ImportError:
    thop = None

from ultralytics.engine.exporter import Exporter


class multihead(BaseModel):
    def __init__(self, cfg_list: list, ch=3, verbose=True, pt_list: list[str]= []):  # model, input channels, number of classes
        """Initialize the YOLOv8 detection model with the given config and parameters."""
        super().__init__()
        # å¦‚æœcfgæ˜¯list, åˆ™åŠ è½½å¤šæ£€æµ‹å¤´æ¨¡å‹
        self.yaml = None #æ­£å¸¸æ£€æµ‹
        self.yaml2 = None #é»‘ç‚¹æ£€æµ‹
        for cfg in cfg_list:
            cfg_yaml = yaml_model_load(cfg)  # cfg dict
            if cfg_yaml["nc"] == 1:
                self.yaml2 = cfg_yaml
            else : self.yaml = cfg_yaml

        # Define model
        # ch = self.yaml["ch"] = self.yaml.get("ch", ch)  # input channels
        # if nc and nc != self.yaml["nc"]:
        #     LOGGER.info(f"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}")
        #     self.yaml["nc"] = nc  # override YAML value
        
        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist
        self.model2, self.save2 = parse_model(deepcopy(self.yaml2), ch=ch, verbose=verbose)
        print(self.save, self.save2)

        # Check if the two models are identical
        self.backbone = compare_sequential_modules(self.model[:-1], self.model2[:-1])
        self.head1 = self.model[-1:]
        self.head2 = self.model2[-1:]


        self.names1 = {i: f"{i}" for i in range(self.yaml["nc"])}  # default names dict
        self.names2 = {i: f"{i}" for i in range(self.yaml2["nc"])}  # default names dict

        self.inplace = self.yaml.get("inplace", True)
        self.end2end = getattr(self.model[-1], "end2end", False)

        # Build strides(å¯¼å‡ºæ—¶éœ€è¦)
        m = self.head1[-1]  # Detect()
        if isinstance(m, Detect):  # includes all Detect subclasses like Segment, Pose, OBB, WorldDetect
            s = 256  # 2x min stride
            m.inplace = self.inplace

            def _forward(x):
                """Performs a forward pass through the model, handling different Detect subclass types accordingly."""
                if self.end2end:
                    return self.forward(x)["one2many"]
                return self.forward(x)[0] if isinstance(m, (Segment, Pose, OBB)) else self.forward(x)

            m.stride = torch.tensor([s / x.shape[-2] for x in _forward(torch.zeros(1, ch, s, s))[0]])  # forward
            self.stride = m.stride
            m.bias_init()  # only run once
        else:
            self.stride = torch.Tensor([32])  # default stride for i.e. RTDETR

        # æ‰‹åŠ¨æ·»åŠ taskå±æ€§
        self.task = "detect"


        # åŠ è½½æ¯ä¸ªå­æ¨¡å—çš„é¢„è®­ç»ƒæƒé‡
        if len(pt_list) != 3:
            raise ValueError("pt_listæ¨¡å—æ•°é‡ä¸æ­£ç¡®")
        else:
            self.load(weights=pt_list[0], submodule=self.backbone)
            self.load(weights=pt_list[1], submodule=self.head1)
            self.load(weights=pt_list[2], submodule=self.head2)


        # # Init weights, biases
        # initialize_weights(self)
        # if verbose:
        #     self.info()
        #     LOGGER.info("")
    
    def load(self, weights, submodule=None, verbose=True):
        if not weights:
            raise ValueError("weightså‚æ•°ä¸èƒ½ä¸ºç©º")
        if str(weights).endswith('.pt'):
            weights = torch.load(weights, map_location='cpu')
        submodel = weights['model'].model if isinstance(weights, dict) else weights
        csd = submodel.float().state_dict()
        csd = intersect_dicts(csd, submodule.state_dict(), exclude=[])  # intersect
        submodule.load_state_dict(csd, strict=False)
        compare_sequential_modules(submodel, self.backbone, True)
        if verbose:
            LOGGER.info(f"Transferred {len(csd)}/{len(submodule.state_dict())} items from pretrained weights")

    def _predict_once(self, x, profile=False, visualize=False, embed=None):
        """
        Perform a forward pass through the network.

        Args:
            x (torch.Tensor): The input tensor to the model.
            profile (bool):  Print the computation time of each layer if True, defaults to False.
            visualize (bool): Save the feature maps of the model if True, defaults to False.
            embed (list, optional): A list of feature vectors/embeddings to return.

        Returns:
            (torch.Tensor): The last output of the model.
        """
        y, dt, embeddings = [], [], []  # outputs
        for m in self.backbone:
            if m.f != -1:  # if not from previous layer
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers
            if profile:
                self._profile_one_layer(m, x, dt)
            x = m(x)  # run
            y.append(x if m.i in self.save or m.i in self.save2 else None)  # save output
            if visualize:
                feature_visualization(x, m.type, m.i, save_dir=visualize)
            if embed and m.i in embed:
                embeddings.append(nn.functional.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1))  # flatten
                if m.i == max(embed):
                    return torch.unbind(torch.cat(embeddings, 1), dim=0)
        for m1, m2 in zip(self.head1, self.head2):
            if m1.f != -1 or m2.f != -1:  # if not from previous layer
                x1 = y[m1.f] if isinstance(m1.f, int) else [x if j == -1 else y[j] for j in m1.f]  # from earlier layers
                x2 = y[m2.f] if isinstance(m2.f, int) else [x if j == -1 else y[j] for j in m2.f]  # from earlier layers
            # if profile:
            #     self._profile_one_layer(m, x, dt)
            # x = m(x) 
            x1, x2 = m1(x1), m2(x2)
        return [x1, x2] #x, 

def parse_model(d, ch, verbose=True):  # model_dict, input_channels(3)
    """Parse a YOLO model.yaml dictionary into a PyTorch model."""
    import ast

    # Args
    max_channels = float("inf")
    nc, act, scales = (d.get(x) for x in ("nc", "activation", "scales"))
    depth, width, kpt_shape = (d.get(x, 1.0) for x in ("depth_multiple", "width_multiple", "kpt_shape"))
    if scales:
        scale = d.get("scale")
        if not scale:
            scale = tuple(scales.keys())[0]
            LOGGER.warning(f"WARNING âš ï¸ no model scale passed. Assuming scale='{scale}'.")
        depth, width, max_channels = scales[scale]

    if act:
        Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()
        if verbose:
            LOGGER.info(f"{colorstr('activation:')} {act}")  # print

    if verbose:
        LOGGER.info(f"\n{'':>3}{'from':>20}{'n':>3}{'params':>10}  {'module':<45}{'arguments':<30}")
    ch = [ch]
    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out
    for i, (f, n, m, args) in enumerate(d["backbone"] + d["head"]):  # from, number, module, args
        m = getattr(torch.nn, m[3:]) if "nn." in m else globals()[m]  # get module
        for j, a in enumerate(args):
            if isinstance(a, str):
                with contextlib.suppress(ValueError):
                    args[j] = locals()[a] if a in locals() else ast.literal_eval(a)

        n = n_ = max(round(n * depth), 1) if n > 1 else n  # depth gain
        if m in {
            Classify,
            Conv,
            ConvTranspose,
            GhostConv,
            Bottleneck,
            GhostBottleneck,
            SPP,
            SPPF,
            DWConv,
            Focus,
            BottleneckCSP,
            C1,
            C2,
            C2f,
            RepNCSPELAN4,
            ELAN1,
            ADown,
            AConv,
            SPPELAN,
            C2fAttn,
            C3,
            C3TR,
            C3Ghost,
            nn.ConvTranspose2d,
            DWConvTranspose2d,
            C3x,
            RepC3,
            PSA,
            SCDown,
            C2fCIB,
        }:
            c1, c2 = ch[f], args[0]
            if c2 != nc:  # if c2 not equal to number of classes (i.e. for Classify() output)
                c2 = make_divisible(min(c2, max_channels) * width, 8)
            if m is C2fAttn:
                args[1] = make_divisible(min(args[1], max_channels // 2) * width, 8)  # embed channels
                args[2] = int(
                    max(round(min(args[2], max_channels // 2 // 32)) * width, 1) if args[2] > 1 else args[2]
                )  # num heads

            args = [c1, c2, *args[1:]]
            if m in {BottleneckCSP, C1, C2, C2f, C2fAttn, C3, C3TR, C3Ghost, C3x, RepC3, C2fCIB}:
                args.insert(2, n)  # number of repeats
                n = 1
        elif m is AIFI:
            args = [ch[f], *args]
        elif m in {HGStem, HGBlock}:
            c1, cm, c2 = ch[f], args[0], args[1]
            args = [c1, cm, c2, *args[2:]]
            if m is HGBlock:
                args.insert(4, n)  # number of repeats
                n = 1
        elif m is ResNetLayer:
            c2 = args[1] if args[3] else args[1] * 4
        elif m is nn.BatchNorm2d:
            args = [ch[f]]
        elif m is Concat:
            c2 = sum(ch[x] for x in f)
        elif m in {Detect, WorldDetect, Segment, Pose, OBB, ImagePoolingAttn, v10Detect}:
            args.append([ch[x] for x in f])
            if m is Segment:
                args[2] = make_divisible(min(args[2], max_channels) * width, 8)
        elif m is RTDETRDecoder:  # special case, channels arg must be passed in index 1
            args.insert(1, [ch[x] for x in f])
        elif m is CBLinear:
            c2 = args[0]
            c1 = ch[f]
            args = [c1, c2, *args[1:]]
        elif m is CBFuse:
            c2 = ch[f[-1]]
        else:
            c2 = ch[f]

        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module
        t = str(m)[8:-2].replace("__main__.", "")  # module type
        m.np = sum(x.numel() for x in m_.parameters())  # number params
        m_.i, m_.f, m_.type = i, f, t  # attach index, 'from' index, type
        if verbose:
            LOGGER.info(f"{i:>3}{str(f):>20}{n_:>3}{m.np:10.0f}  {t:<45}{str(args):<30}")  # print
        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist
        layers.append(m_)
        if i == 0:
            ch = []
        ch.append(c2)
    return nn.Sequential(*layers), sorted(save)

def compare_sequential_modules(seq1: nn.Sequential, seq2: nn.Sequential, compare_weights: bool = False) -> nn.Sequential:
    # æ£€æŸ¥ä¸¤ä¸ªæ¨¡å—çš„å±‚æ•°æ˜¯å¦ç›¸åŒ
    if len(seq1) != len(seq2):
        raise ValueError("The two nn.Sequential modules have different number of layers.")
    
    # é€å±‚æ¯”è¾ƒä¸¤ä¸ªæ¨¡å—çš„å‚æ•°
    for layer1, layer2 in zip(seq1, seq2):
        # æ¯”è¾ƒæ¯ä¸€å±‚çš„å‚æ•°
        for param1, param2 in zip(layer1.parameters(), layer2.parameters()):
            if not param1.shape==param2.shape:
                raise ValueError("ä¸æ˜¯ç›¸åŒçš„backbone")
            if compare_weights and not torch.equal(param1, param2):
                print("æ¨¡å—çš„å‚æ•°ä¸ç›¸åŒ")
    
    print("æ˜¯ç›¸åŒç»“æ„çš„backbone.")
    return seq1

def export(
    model,# self,
    **kwargs,
) -> str:
    """
    Exports the model to a different format suitable for deployment.

    This method facilitates the export of the model to various formats (e.g., ONNX, TorchScript) for deployment
    purposes. It uses the 'Exporter' class for the export process, combining model-specific overrides, method
    defaults, and any additional arguments provided.

    Args:
        **kwargs (Dict): Arbitrary keyword arguments to customize the export process. These are combined with
            the model's overrides and method defaults. Common arguments include:
            format (str): Export format (e.g., 'onnx', 'engine', 'coreml').
            half (bool): Export model in half-precision.
            int8 (bool): Export model in int8 precision.
            device (str): Device to run the export on.
            workspace (int): Maximum memory workspace size for TensorRT engines.
            nms (bool): Add Non-Maximum Suppression (NMS) module to model.
            simplify (bool): Simplify ONNX model.

    Returns:
        (str): The path to the exported model file.

    Raises:
        AssertionError: If the model is not a PyTorch model.
        ValueError: If an unsupported export format is specified.
        RuntimeError: If the export process fails due to errors.

    Examples:
        >>> model = YOLO("yolov8n.pt")
        >>> model.export(format="onnx", dynamic=True, simplify=True)
        'path/to/exported/model.onnx'
    """
    # self._check_is_pytorch_model()
    # from .exporter import Exporter

    custom = {
        "imgsz": 640,
        "batch": 1,
        "data": None,
        "device": None,  # reset to avoid multi-GPU errors
        "verbose": False,
    }  # method defaults
    args = {**custom, **kwargs, "mode": "export"}  # highest priority args on the right
    return Exporter(overrides=args, _callbacks=None)(model=model)

if __name__ == "__main__":
    cfg1 = '/home/tyjt/æ¡Œé¢/ultralytics/custom_model_config/yolov8-ghost-onlyp2.yaml'
    cfg2 = '/home/tyjt/æ¡Œé¢/ultralytics/custom_model_config/yolov8-ghost-p3-p5.yaml'
    pt1 = ''
    pt2 = ''
    pt3 = ''
    model = multihead(cfg_list=[cfg1, cfg2], ch=3, pt_list=[pt1, pt2, pt3])

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    # model.load(weights='/home/tyjt/æ¡Œé¢/ultralytics/runs/detect/train8/weights/backbone_module_epoch0.pt', submodule=model.backbone)

    # ä¸åŒçš„æ¨¡å¼äº§ç”Ÿçš„ä¸åŒçš„è¾“å‡º(è®­ç»ƒï¼Œæ¨ç†ï¼Œå¯¼å‡º)
    # model.eval()
    # x = torch.randn(1, 3, 640, 640)
    # head_opt = model(x)
    # print(type(head_opt), len(head_opt))
    # print(type(head_opt[0]),len(head_opt[0]))
    # print(type(head_opt[1]),len(head_opt[1]))
    # print(head_opt[0][0].shape, head_opt[1][0].shape)
    # print([x.shape for x in head_opt[0][1]])

    # export(model, format="onnx")